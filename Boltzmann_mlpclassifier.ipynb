{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Restricted Boltzmann Machine (RBM)\n",
        "## Is a type of artificial neural network used for various machine learning tasks\n",
        "### In this notebook, it was used for dimensionality reduction, and  \n",
        "### MLPClassifier was used to compare results between with and without reduction\n",
        "MLClassifier is multi-layer perceptro classifier\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
      ],
      "metadata": {
        "id": "YU02PhzYBPvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7nJAT_aJA_FQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "from sklearn import datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the digits dataset (MNIST)\n",
        "digits = datasets.load_digits()\n",
        "# convert pixes to array\n",
        "X = np.asarray(digits.data, 'float32')\n",
        "# Split it into training and testing datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, digits.target, test_size=0.2, random_state=0)\n",
        "# Scale the data\n",
        "X_train = MinMaxScaler().fit_transform(X_train)\n",
        "X_test = MinMaxScaler().fit_transform(X_test)\n",
        "# Models we will use\n",
        "# hyperparameter\n",
        "n_components = 50 # how many dimensions will be results as a result of RBM\n",
        "learning_rate = 0.01\n",
        "n_iter = 25\n",
        "max_iter = 1000\n",
        "# how to calculate hidden_layer_sizes 64 with dimensionlity + 10 output (softmax probabilty) = 74/2\n",
        "model_mlp = MLPClassifier(hidden_layer_sizes = (37, 37),\n",
        "                          activation = 'relu',\n",
        "                          solver = 'adam',\n",
        "                          max_iter = max_iter,\n",
        "                          verbose=True )\n",
        "\n",
        "model_rbm = BernoulliRBM(random_state=0,\n",
        "                   n_components = n_components,\n",
        "                   n_iter=n_iter,\n",
        "                   learning_rate=learning_rate,\n",
        "                   verbose=True )\n",
        "\n",
        "# train in RBM, to dimensionality reduction\n",
        "# model.fit(X_train)\n",
        "\n",
        "# create pipeline each steps is model to fit: first reduce second classifier with reduction\n",
        "pipeline_mlp = Pipeline(steps=[('rbm', model_rbm), ('mlp', model_mlp)])\n",
        "pipeline_mlp.fit(X_train, Y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fOn6c3Hv4CPR",
        "outputId": "55e1d37d-7760-472c-f5b7-30be16928a80"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -26.83, time = 0.04s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -26.30, time = 0.10s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -26.15, time = 0.10s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -26.18, time = 0.09s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -26.10, time = 0.12s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -26.02, time = 0.09s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -26.00, time = 0.11s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -25.99, time = 0.10s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -26.04, time = 0.10s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -26.03, time = 0.09s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -26.05, time = 0.11s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -26.02, time = 0.09s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -26.01, time = 0.13s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -25.96, time = 0.10s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -25.79, time = 0.10s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -25.95, time = 0.09s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -25.70, time = 0.11s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -25.64, time = 0.11s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -25.54, time = 0.11s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -25.50, time = 0.04s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -25.19, time = 0.06s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -24.95, time = 0.06s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -24.64, time = 0.06s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -24.37, time = 0.05s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -24.32, time = 0.05s\n",
            "Iteration 1, loss = 2.30998941\n",
            "Iteration 2, loss = 2.25855415\n",
            "Iteration 3, loss = 2.22389233\n",
            "Iteration 4, loss = 2.18901303\n",
            "Iteration 5, loss = 2.15190990\n",
            "Iteration 6, loss = 2.11000748\n",
            "Iteration 7, loss = 2.06223549\n",
            "Iteration 8, loss = 2.00740098\n",
            "Iteration 9, loss = 1.94765759\n",
            "Iteration 10, loss = 1.88432259\n",
            "Iteration 11, loss = 1.81867943\n",
            "Iteration 12, loss = 1.75374493\n",
            "Iteration 13, loss = 1.68553240\n",
            "Iteration 14, loss = 1.61808394\n",
            "Iteration 15, loss = 1.55144263\n",
            "Iteration 16, loss = 1.48941140\n",
            "Iteration 17, loss = 1.43346577\n",
            "Iteration 18, loss = 1.37975564\n",
            "Iteration 19, loss = 1.33060448\n",
            "Iteration 20, loss = 1.29049091\n",
            "Iteration 21, loss = 1.24891831\n",
            "Iteration 22, loss = 1.21719145\n",
            "Iteration 23, loss = 1.18450729\n",
            "Iteration 24, loss = 1.15685023\n",
            "Iteration 25, loss = 1.13789968\n",
            "Iteration 26, loss = 1.11413352\n",
            "Iteration 27, loss = 1.09591409\n",
            "Iteration 28, loss = 1.08203214\n",
            "Iteration 29, loss = 1.06497281\n",
            "Iteration 30, loss = 1.04650052\n",
            "Iteration 31, loss = 1.03718006\n",
            "Iteration 32, loss = 1.02199809\n",
            "Iteration 33, loss = 1.01135104\n",
            "Iteration 34, loss = 0.99914809\n",
            "Iteration 35, loss = 0.98798825\n",
            "Iteration 36, loss = 0.97512962\n",
            "Iteration 37, loss = 0.96830621\n",
            "Iteration 38, loss = 0.95619545\n",
            "Iteration 39, loss = 0.94572970\n",
            "Iteration 40, loss = 0.93714898\n",
            "Iteration 41, loss = 0.92556588\n",
            "Iteration 42, loss = 0.91487420\n",
            "Iteration 43, loss = 0.91175424\n",
            "Iteration 44, loss = 0.90041618\n",
            "Iteration 45, loss = 0.89354504\n",
            "Iteration 46, loss = 0.88170426\n",
            "Iteration 47, loss = 0.87974873\n",
            "Iteration 48, loss = 0.86604458\n",
            "Iteration 49, loss = 0.85780644\n",
            "Iteration 50, loss = 0.84947756\n",
            "Iteration 51, loss = 0.84164622\n",
            "Iteration 52, loss = 0.83689724\n",
            "Iteration 53, loss = 0.82843805\n",
            "Iteration 54, loss = 0.82464041\n",
            "Iteration 55, loss = 0.81726062\n",
            "Iteration 56, loss = 0.81388832\n",
            "Iteration 57, loss = 0.81712212\n",
            "Iteration 58, loss = 0.80469778\n",
            "Iteration 59, loss = 0.80313635\n",
            "Iteration 60, loss = 0.79958247\n",
            "Iteration 61, loss = 0.79681051\n",
            "Iteration 62, loss = 0.79154375\n",
            "Iteration 63, loss = 0.78491954\n",
            "Iteration 64, loss = 0.78308071\n",
            "Iteration 65, loss = 0.78489983\n",
            "Iteration 66, loss = 0.77768483\n",
            "Iteration 67, loss = 0.77561013\n",
            "Iteration 68, loss = 0.77373475\n",
            "Iteration 69, loss = 0.77029574\n",
            "Iteration 70, loss = 0.76961044\n",
            "Iteration 71, loss = 0.76298337\n",
            "Iteration 72, loss = 0.76044800\n",
            "Iteration 73, loss = 0.76119775\n",
            "Iteration 74, loss = 0.75980853\n",
            "Iteration 75, loss = 0.76162196\n",
            "Iteration 76, loss = 0.75723798\n",
            "Iteration 77, loss = 0.75067260\n",
            "Iteration 78, loss = 0.74951038\n",
            "Iteration 79, loss = 0.75176043\n",
            "Iteration 80, loss = 0.74805305\n",
            "Iteration 81, loss = 0.74766597\n",
            "Iteration 82, loss = 0.74356377\n",
            "Iteration 83, loss = 0.74254419\n",
            "Iteration 84, loss = 0.74116272\n",
            "Iteration 85, loss = 0.73980764\n",
            "Iteration 86, loss = 0.74313478\n",
            "Iteration 87, loss = 0.74006672\n",
            "Iteration 88, loss = 0.74067760\n",
            "Iteration 89, loss = 0.73479812\n",
            "Iteration 90, loss = 0.73566464\n",
            "Iteration 91, loss = 0.73432035\n",
            "Iteration 92, loss = 0.73369723\n",
            "Iteration 93, loss = 0.73605975\n",
            "Iteration 94, loss = 0.72739231\n",
            "Iteration 95, loss = 0.72528509\n",
            "Iteration 96, loss = 0.72579174\n",
            "Iteration 97, loss = 0.72644961\n",
            "Iteration 98, loss = 0.72383619\n",
            "Iteration 99, loss = 0.72606627\n",
            "Iteration 100, loss = 0.72262203\n",
            "Iteration 101, loss = 0.72046887\n",
            "Iteration 102, loss = 0.72179584\n",
            "Iteration 103, loss = 0.71873366\n",
            "Iteration 104, loss = 0.71453212\n",
            "Iteration 105, loss = 0.71460039\n",
            "Iteration 106, loss = 0.71518037\n",
            "Iteration 107, loss = 0.71305423\n",
            "Iteration 108, loss = 0.71274334\n",
            "Iteration 109, loss = 0.71412892\n",
            "Iteration 110, loss = 0.71129645\n",
            "Iteration 111, loss = 0.70992521\n",
            "Iteration 112, loss = 0.71292284\n",
            "Iteration 113, loss = 0.72019432\n",
            "Iteration 114, loss = 0.70732000\n",
            "Iteration 115, loss = 0.71272282\n",
            "Iteration 116, loss = 0.71114835\n",
            "Iteration 117, loss = 0.70497501\n",
            "Iteration 118, loss = 0.70540790\n",
            "Iteration 119, loss = 0.70220223\n",
            "Iteration 120, loss = 0.70187566\n",
            "Iteration 121, loss = 0.70560923\n",
            "Iteration 122, loss = 0.70702981\n",
            "Iteration 123, loss = 0.70018787\n",
            "Iteration 124, loss = 0.69903180\n",
            "Iteration 125, loss = 0.69598591\n",
            "Iteration 126, loss = 0.69846445\n",
            "Iteration 127, loss = 0.69894477\n",
            "Iteration 128, loss = 0.69561478\n",
            "Iteration 129, loss = 0.69729907\n",
            "Iteration 130, loss = 0.69527177\n",
            "Iteration 131, loss = 0.69397689\n",
            "Iteration 132, loss = 0.69404025\n",
            "Iteration 133, loss = 0.69425673\n",
            "Iteration 134, loss = 0.69209880\n",
            "Iteration 135, loss = 0.69321601\n",
            "Iteration 136, loss = 0.68919122\n",
            "Iteration 137, loss = 0.68857462\n",
            "Iteration 138, loss = 0.69008173\n",
            "Iteration 139, loss = 0.68713683\n",
            "Iteration 140, loss = 0.68769049\n",
            "Iteration 141, loss = 0.68492358\n",
            "Iteration 142, loss = 0.68640937\n",
            "Iteration 143, loss = 0.68523565\n",
            "Iteration 144, loss = 0.68106987\n",
            "Iteration 145, loss = 0.68081539\n",
            "Iteration 146, loss = 0.68265519\n",
            "Iteration 147, loss = 0.67955517\n",
            "Iteration 148, loss = 0.67949814\n",
            "Iteration 149, loss = 0.68452666\n",
            "Iteration 150, loss = 0.68731674\n",
            "Iteration 151, loss = 0.67544883\n",
            "Iteration 152, loss = 0.68219479\n",
            "Iteration 153, loss = 0.67946355\n",
            "Iteration 154, loss = 0.67933746\n",
            "Iteration 155, loss = 0.67675195\n",
            "Iteration 156, loss = 0.67744601\n",
            "Iteration 157, loss = 0.67710086\n",
            "Iteration 158, loss = 0.67332551\n",
            "Iteration 159, loss = 0.66930965\n",
            "Iteration 160, loss = 0.66871373\n",
            "Iteration 161, loss = 0.66850279\n",
            "Iteration 162, loss = 0.66875216\n",
            "Iteration 163, loss = 0.66677462\n",
            "Iteration 164, loss = 0.66664376\n",
            "Iteration 165, loss = 0.67069878\n",
            "Iteration 166, loss = 0.66706571\n",
            "Iteration 167, loss = 0.66881665\n",
            "Iteration 168, loss = 0.67471902\n",
            "Iteration 169, loss = 0.67316274\n",
            "Iteration 170, loss = 0.66430666\n",
            "Iteration 171, loss = 0.66197183\n",
            "Iteration 172, loss = 0.66695302\n",
            "Iteration 173, loss = 0.66388849\n",
            "Iteration 174, loss = 0.66033639\n",
            "Iteration 175, loss = 0.65787936\n",
            "Iteration 176, loss = 0.65585453\n",
            "Iteration 177, loss = 0.65607110\n",
            "Iteration 178, loss = 0.65622177\n",
            "Iteration 179, loss = 0.65498164\n",
            "Iteration 180, loss = 0.65885183\n",
            "Iteration 181, loss = 0.65711262\n",
            "Iteration 182, loss = 0.65673651\n",
            "Iteration 183, loss = 0.65330078\n",
            "Iteration 184, loss = 0.65307666\n",
            "Iteration 185, loss = 0.66749660\n",
            "Iteration 186, loss = 0.65907196\n",
            "Iteration 187, loss = 0.65356340\n",
            "Iteration 188, loss = 0.64607646\n",
            "Iteration 189, loss = 0.64757231\n",
            "Iteration 190, loss = 0.65469325\n",
            "Iteration 191, loss = 0.65531863\n",
            "Iteration 192, loss = 0.65094141\n",
            "Iteration 193, loss = 0.64732009\n",
            "Iteration 194, loss = 0.64450196\n",
            "Iteration 195, loss = 0.64476781\n",
            "Iteration 196, loss = 0.64171105\n",
            "Iteration 197, loss = 0.64504446\n",
            "Iteration 198, loss = 0.63931484\n",
            "Iteration 199, loss = 0.63968032\n",
            "Iteration 200, loss = 0.63605351\n",
            "Iteration 201, loss = 0.63963547\n",
            "Iteration 202, loss = 0.63614948\n",
            "Iteration 203, loss = 0.63612243\n",
            "Iteration 204, loss = 0.63718378\n",
            "Iteration 205, loss = 0.63797499\n",
            "Iteration 206, loss = 0.63491723\n",
            "Iteration 207, loss = 0.63360761\n",
            "Iteration 208, loss = 0.63191828\n",
            "Iteration 209, loss = 0.63310902\n",
            "Iteration 210, loss = 0.63611324\n",
            "Iteration 211, loss = 0.64487340\n",
            "Iteration 212, loss = 0.63179243\n",
            "Iteration 213, loss = 0.62860930\n",
            "Iteration 214, loss = 0.62688895\n",
            "Iteration 215, loss = 0.63231395\n",
            "Iteration 216, loss = 0.62941647\n",
            "Iteration 217, loss = 0.62503191\n",
            "Iteration 218, loss = 0.62470177\n",
            "Iteration 219, loss = 0.62710313\n",
            "Iteration 220, loss = 0.62275231\n",
            "Iteration 221, loss = 0.62456680\n",
            "Iteration 222, loss = 0.61954476\n",
            "Iteration 223, loss = 0.61934334\n",
            "Iteration 224, loss = 0.61652628\n",
            "Iteration 225, loss = 0.61678756\n",
            "Iteration 226, loss = 0.61744650\n",
            "Iteration 227, loss = 0.61929253\n",
            "Iteration 228, loss = 0.62285871\n",
            "Iteration 229, loss = 0.61851627\n",
            "Iteration 230, loss = 0.61290388\n",
            "Iteration 231, loss = 0.61066440\n",
            "Iteration 232, loss = 0.61215135\n",
            "Iteration 233, loss = 0.60913322\n",
            "Iteration 234, loss = 0.61213930\n",
            "Iteration 235, loss = 0.61630615\n",
            "Iteration 236, loss = 0.60929717\n",
            "Iteration 237, loss = 0.61199905\n",
            "Iteration 238, loss = 0.61292925\n",
            "Iteration 239, loss = 0.61139066\n",
            "Iteration 240, loss = 0.60685513\n",
            "Iteration 241, loss = 0.61227546\n",
            "Iteration 242, loss = 0.60564321\n",
            "Iteration 243, loss = 0.60163895\n",
            "Iteration 244, loss = 0.60767693\n",
            "Iteration 245, loss = 0.60696361\n",
            "Iteration 246, loss = 0.60526504\n",
            "Iteration 247, loss = 0.60361763\n",
            "Iteration 248, loss = 0.60026579\n",
            "Iteration 249, loss = 0.59823359\n",
            "Iteration 250, loss = 0.59552727\n",
            "Iteration 251, loss = 0.59584263\n",
            "Iteration 252, loss = 0.59425402\n",
            "Iteration 253, loss = 0.59377255\n",
            "Iteration 254, loss = 0.59319555\n",
            "Iteration 255, loss = 0.59363110\n",
            "Iteration 256, loss = 0.59347157\n",
            "Iteration 257, loss = 0.58943565\n",
            "Iteration 258, loss = 0.58859560\n",
            "Iteration 259, loss = 0.58878744\n",
            "Iteration 260, loss = 0.59306484\n",
            "Iteration 261, loss = 0.58975409\n",
            "Iteration 262, loss = 0.59067771\n",
            "Iteration 263, loss = 0.58571631\n",
            "Iteration 264, loss = 0.58479292\n",
            "Iteration 265, loss = 0.58421547\n",
            "Iteration 266, loss = 0.58342397\n",
            "Iteration 267, loss = 0.58043607\n",
            "Iteration 268, loss = 0.58026974\n",
            "Iteration 269, loss = 0.58207281\n",
            "Iteration 270, loss = 0.57731400\n",
            "Iteration 271, loss = 0.57624540\n",
            "Iteration 272, loss = 0.57862086\n",
            "Iteration 273, loss = 0.57656512\n",
            "Iteration 274, loss = 0.57954373\n",
            "Iteration 275, loss = 0.58144766\n",
            "Iteration 276, loss = 0.57779339\n",
            "Iteration 277, loss = 0.57559690\n",
            "Iteration 278, loss = 0.57592809\n",
            "Iteration 279, loss = 0.57457765\n",
            "Iteration 280, loss = 0.56996060\n",
            "Iteration 281, loss = 0.57148278\n",
            "Iteration 282, loss = 0.57108787\n",
            "Iteration 283, loss = 0.57723191\n",
            "Iteration 284, loss = 0.56988045\n",
            "Iteration 285, loss = 0.56972908\n",
            "Iteration 286, loss = 0.57046857\n",
            "Iteration 287, loss = 0.56791359\n",
            "Iteration 288, loss = 0.57072960\n",
            "Iteration 289, loss = 0.56274344\n",
            "Iteration 290, loss = 0.56215627\n",
            "Iteration 291, loss = 0.56149633\n",
            "Iteration 292, loss = 0.55959626\n",
            "Iteration 293, loss = 0.55708648\n",
            "Iteration 294, loss = 0.55783187\n",
            "Iteration 295, loss = 0.55743876\n",
            "Iteration 296, loss = 0.55526552\n",
            "Iteration 297, loss = 0.55393386\n",
            "Iteration 298, loss = 0.55446589\n",
            "Iteration 299, loss = 0.55138466\n",
            "Iteration 300, loss = 0.55569846\n",
            "Iteration 301, loss = 0.55777159\n",
            "Iteration 302, loss = 0.56077486\n",
            "Iteration 303, loss = 0.56210304\n",
            "Iteration 304, loss = 0.56319809\n",
            "Iteration 305, loss = 0.55106738\n",
            "Iteration 306, loss = 0.55143615\n",
            "Iteration 307, loss = 0.55253578\n",
            "Iteration 308, loss = 0.54864294\n",
            "Iteration 309, loss = 0.55159732\n",
            "Iteration 310, loss = 0.54770699\n",
            "Iteration 311, loss = 0.54603961\n",
            "Iteration 312, loss = 0.54693722\n",
            "Iteration 313, loss = 0.54289435\n",
            "Iteration 314, loss = 0.54169913\n",
            "Iteration 315, loss = 0.54253569\n",
            "Iteration 316, loss = 0.53892699\n",
            "Iteration 317, loss = 0.53908677\n",
            "Iteration 318, loss = 0.53725471\n",
            "Iteration 319, loss = 0.53595607\n",
            "Iteration 320, loss = 0.53386945\n",
            "Iteration 321, loss = 0.53607551\n",
            "Iteration 322, loss = 0.53608337\n",
            "Iteration 323, loss = 0.53188464\n",
            "Iteration 324, loss = 0.53314960\n",
            "Iteration 325, loss = 0.53204271\n",
            "Iteration 326, loss = 0.53009196\n",
            "Iteration 327, loss = 0.53645226\n",
            "Iteration 328, loss = 0.52700225\n",
            "Iteration 329, loss = 0.52872440\n",
            "Iteration 330, loss = 0.52793131\n",
            "Iteration 331, loss = 0.52887100\n",
            "Iteration 332, loss = 0.53181686\n",
            "Iteration 333, loss = 0.52526758\n",
            "Iteration 334, loss = 0.52419083\n",
            "Iteration 335, loss = 0.52503925\n",
            "Iteration 336, loss = 0.53104823\n",
            "Iteration 337, loss = 0.52422457\n",
            "Iteration 338, loss = 0.52158958\n",
            "Iteration 339, loss = 0.52397873\n",
            "Iteration 340, loss = 0.51840948\n",
            "Iteration 341, loss = 0.51701398\n",
            "Iteration 342, loss = 0.52053625\n",
            "Iteration 343, loss = 0.52119553\n",
            "Iteration 344, loss = 0.51598591\n",
            "Iteration 345, loss = 0.51966022\n",
            "Iteration 346, loss = 0.51601913\n",
            "Iteration 347, loss = 0.51584859\n",
            "Iteration 348, loss = 0.51308365\n",
            "Iteration 349, loss = 0.52778666\n",
            "Iteration 350, loss = 0.51912020\n",
            "Iteration 351, loss = 0.51898214\n",
            "Iteration 352, loss = 0.51259523\n",
            "Iteration 353, loss = 0.50876593\n",
            "Iteration 354, loss = 0.50973113\n",
            "Iteration 355, loss = 0.51104771\n",
            "Iteration 356, loss = 0.51829790\n",
            "Iteration 357, loss = 0.51065198\n",
            "Iteration 358, loss = 0.50462907\n",
            "Iteration 359, loss = 0.50342815\n",
            "Iteration 360, loss = 0.50420754\n",
            "Iteration 361, loss = 0.50579315\n",
            "Iteration 362, loss = 0.50075253\n",
            "Iteration 363, loss = 0.50121080\n",
            "Iteration 364, loss = 0.49929257\n",
            "Iteration 365, loss = 0.50814876\n",
            "Iteration 366, loss = 0.49997037\n",
            "Iteration 367, loss = 0.49931847\n",
            "Iteration 368, loss = 0.50232856\n",
            "Iteration 369, loss = 0.49968318\n",
            "Iteration 370, loss = 0.49435827\n",
            "Iteration 371, loss = 0.49665426\n",
            "Iteration 372, loss = 0.49389868\n",
            "Iteration 373, loss = 0.49443019\n",
            "Iteration 374, loss = 0.49164637\n",
            "Iteration 375, loss = 0.49588074\n",
            "Iteration 376, loss = 0.48988186\n",
            "Iteration 377, loss = 0.49409409\n",
            "Iteration 378, loss = 0.49345634\n",
            "Iteration 379, loss = 0.49351581\n",
            "Iteration 380, loss = 0.49203662\n",
            "Iteration 381, loss = 0.48872925\n",
            "Iteration 382, loss = 0.48987950\n",
            "Iteration 383, loss = 0.48617979\n",
            "Iteration 384, loss = 0.48510327\n",
            "Iteration 385, loss = 0.48548165\n",
            "Iteration 386, loss = 0.48663399\n",
            "Iteration 387, loss = 0.48612313\n",
            "Iteration 388, loss = 0.48189363\n",
            "Iteration 389, loss = 0.48252221\n",
            "Iteration 390, loss = 0.48339757\n",
            "Iteration 391, loss = 0.48636363\n",
            "Iteration 392, loss = 0.48108604\n",
            "Iteration 393, loss = 0.48243596\n",
            "Iteration 394, loss = 0.47887149\n",
            "Iteration 395, loss = 0.48116087\n",
            "Iteration 396, loss = 0.47885557\n",
            "Iteration 397, loss = 0.47791257\n",
            "Iteration 398, loss = 0.47733747\n",
            "Iteration 399, loss = 0.47562305\n",
            "Iteration 400, loss = 0.47511762\n",
            "Iteration 401, loss = 0.47748638\n",
            "Iteration 402, loss = 0.47407724\n",
            "Iteration 403, loss = 0.47472191\n",
            "Iteration 404, loss = 0.47748052\n",
            "Iteration 405, loss = 0.47390147\n",
            "Iteration 406, loss = 0.47485985\n",
            "Iteration 407, loss = 0.47449881\n",
            "Iteration 408, loss = 0.47174516\n",
            "Iteration 409, loss = 0.47295701\n",
            "Iteration 410, loss = 0.47213312\n",
            "Iteration 411, loss = 0.47401301\n",
            "Iteration 412, loss = 0.47431781\n",
            "Iteration 413, loss = 0.47361309\n",
            "Iteration 414, loss = 0.46959912\n",
            "Iteration 415, loss = 0.46829479\n",
            "Iteration 416, loss = 0.46956543\n",
            "Iteration 417, loss = 0.46685111\n",
            "Iteration 418, loss = 0.46725512\n",
            "Iteration 419, loss = 0.46830677\n",
            "Iteration 420, loss = 0.46505438\n",
            "Iteration 421, loss = 0.46596840\n",
            "Iteration 422, loss = 0.46383146\n",
            "Iteration 423, loss = 0.46334815\n",
            "Iteration 424, loss = 0.46622514\n",
            "Iteration 425, loss = 0.46573524\n",
            "Iteration 426, loss = 0.46547389\n",
            "Iteration 427, loss = 0.46556393\n",
            "Iteration 428, loss = 0.46017939\n",
            "Iteration 429, loss = 0.46038926\n",
            "Iteration 430, loss = 0.45941470\n",
            "Iteration 431, loss = 0.46255779\n",
            "Iteration 432, loss = 0.45749664\n",
            "Iteration 433, loss = 0.46200151\n",
            "Iteration 434, loss = 0.45683140\n",
            "Iteration 435, loss = 0.45558647\n",
            "Iteration 436, loss = 0.45986276\n",
            "Iteration 437, loss = 0.45819668\n",
            "Iteration 438, loss = 0.45935834\n",
            "Iteration 439, loss = 0.45741297\n",
            "Iteration 440, loss = 0.45945393\n",
            "Iteration 441, loss = 0.46663707\n",
            "Iteration 442, loss = 0.46004543\n",
            "Iteration 443, loss = 0.45216233\n",
            "Iteration 444, loss = 0.45099530\n",
            "Iteration 445, loss = 0.45462451\n",
            "Iteration 446, loss = 0.45822674\n",
            "Iteration 447, loss = 0.45341161\n",
            "Iteration 448, loss = 0.45203467\n",
            "Iteration 449, loss = 0.46143952\n",
            "Iteration 450, loss = 0.45488311\n",
            "Iteration 451, loss = 0.45209178\n",
            "Iteration 452, loss = 0.45095567\n",
            "Iteration 453, loss = 0.45632473\n",
            "Iteration 454, loss = 0.45444633\n",
            "Iteration 455, loss = 0.45540614\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('rbm',\n",
              "                 BernoulliRBM(learning_rate=0.01, n_components=50, n_iter=25,\n",
              "                              random_state=0, verbose=True)),\n",
              "                ('mlp',\n",
              "                 MLPClassifier(hidden_layer_sizes=(37, 37), max_iter=1000,\n",
              "                               verbose=True))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;rbm&#x27;,\n",
              "                 BernoulliRBM(learning_rate=0.01, n_components=50, n_iter=25,\n",
              "                              random_state=0, verbose=True)),\n",
              "                (&#x27;mlp&#x27;,\n",
              "                 MLPClassifier(hidden_layer_sizes=(37, 37), max_iter=1000,\n",
              "                               verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;rbm&#x27;,\n",
              "                 BernoulliRBM(learning_rate=0.01, n_components=50, n_iter=25,\n",
              "                              random_state=0, verbose=True)),\n",
              "                (&#x27;mlp&#x27;,\n",
              "                 MLPClassifier(hidden_layer_sizes=(37, 37), max_iter=1000,\n",
              "                               verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliRBM</label><div class=\"sk-toggleable__content\"><pre>BernoulliRBM(learning_rate=0.01, n_components=50, n_iter=25, random_state=0,\n",
              "             verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(37, 37), max_iter=1000, verbose=True)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model used dimensionality reduction\n",
        "print(\"MLP:\")\n",
        "Y_pred_mlp = pipeline_mlp.predict(X_test)\n",
        "print(accuracy_score(Y_test, Y_pred_mlp))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJo3QfhcZHrr",
        "outputId": "38ebd317-38f3-49c9-a757-7445fb912b94"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP:\n",
            "0.8222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just MLP Classifier\n",
        "model_mlp.fit(X_train, Y_train)\n",
        "Y_pred_mlp = model_mlp.predict(X_test)\n",
        "print(accuracy_score(Y_test, Y_pred_mlp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cwJKaSaCjT",
        "outputId": "7a01d979-1fe7-4d06-ee33-d30a07960760"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31469436\n",
            "Iteration 2, loss = 2.24206251\n",
            "Iteration 3, loss = 2.17548799\n",
            "Iteration 4, loss = 2.09609875\n",
            "Iteration 5, loss = 2.00072621\n",
            "Iteration 6, loss = 1.88380638\n",
            "Iteration 7, loss = 1.74293203\n",
            "Iteration 8, loss = 1.58681345\n",
            "Iteration 9, loss = 1.42795691\n",
            "Iteration 10, loss = 1.26859613\n",
            "Iteration 11, loss = 1.12079800\n",
            "Iteration 12, loss = 0.98862235\n",
            "Iteration 13, loss = 0.86764327\n",
            "Iteration 14, loss = 0.76740314\n",
            "Iteration 15, loss = 0.67678011\n",
            "Iteration 16, loss = 0.60130764\n",
            "Iteration 17, loss = 0.53653960\n",
            "Iteration 18, loss = 0.48236430\n",
            "Iteration 19, loss = 0.43416380\n",
            "Iteration 20, loss = 0.39372123\n",
            "Iteration 21, loss = 0.36087048\n",
            "Iteration 22, loss = 0.33183079\n",
            "Iteration 23, loss = 0.30685560\n",
            "Iteration 24, loss = 0.28820958\n",
            "Iteration 25, loss = 0.26901741\n",
            "Iteration 26, loss = 0.25485591\n",
            "Iteration 27, loss = 0.24061216\n",
            "Iteration 28, loss = 0.23088110\n",
            "Iteration 29, loss = 0.21844481\n",
            "Iteration 30, loss = 0.20746475\n",
            "Iteration 31, loss = 0.20006394\n",
            "Iteration 32, loss = 0.19150083\n",
            "Iteration 33, loss = 0.18434958\n",
            "Iteration 34, loss = 0.17763805\n",
            "Iteration 35, loss = 0.17324354\n",
            "Iteration 36, loss = 0.16682784\n",
            "Iteration 37, loss = 0.16146319\n",
            "Iteration 38, loss = 0.15598161\n",
            "Iteration 39, loss = 0.15175646\n",
            "Iteration 40, loss = 0.14376953\n",
            "Iteration 41, loss = 0.13909639\n",
            "Iteration 42, loss = 0.13858338\n",
            "Iteration 43, loss = 0.13323708\n",
            "Iteration 44, loss = 0.12953397\n",
            "Iteration 45, loss = 0.12747805\n",
            "Iteration 46, loss = 0.12180726\n",
            "Iteration 47, loss = 0.11787689\n",
            "Iteration 48, loss = 0.11509919\n",
            "Iteration 49, loss = 0.11182253\n",
            "Iteration 50, loss = 0.10883565\n",
            "Iteration 51, loss = 0.10601345\n",
            "Iteration 52, loss = 0.10636846\n",
            "Iteration 53, loss = 0.10261509\n",
            "Iteration 54, loss = 0.09995347\n",
            "Iteration 55, loss = 0.09838135\n",
            "Iteration 56, loss = 0.09502491\n",
            "Iteration 57, loss = 0.09373778\n",
            "Iteration 58, loss = 0.09067695\n",
            "Iteration 59, loss = 0.09014473\n",
            "Iteration 60, loss = 0.08669185\n",
            "Iteration 61, loss = 0.08472152\n",
            "Iteration 62, loss = 0.08308350\n",
            "Iteration 63, loss = 0.08094973\n",
            "Iteration 64, loss = 0.07905596\n",
            "Iteration 65, loss = 0.07790452\n",
            "Iteration 66, loss = 0.07673127\n",
            "Iteration 67, loss = 0.07619908\n",
            "Iteration 68, loss = 0.07261542\n",
            "Iteration 69, loss = 0.07261834\n",
            "Iteration 70, loss = 0.06976618\n",
            "Iteration 71, loss = 0.06845442\n",
            "Iteration 72, loss = 0.06704188\n",
            "Iteration 73, loss = 0.06567584\n",
            "Iteration 74, loss = 0.06439634\n",
            "Iteration 75, loss = 0.06592536\n",
            "Iteration 76, loss = 0.06371561\n",
            "Iteration 77, loss = 0.06187060\n",
            "Iteration 78, loss = 0.06004577\n",
            "Iteration 79, loss = 0.05908891\n",
            "Iteration 80, loss = 0.05740339\n",
            "Iteration 81, loss = 0.05667566\n",
            "Iteration 82, loss = 0.05663999\n",
            "Iteration 83, loss = 0.05598497\n",
            "Iteration 84, loss = 0.05390702\n",
            "Iteration 85, loss = 0.05286331\n",
            "Iteration 86, loss = 0.05160487\n",
            "Iteration 87, loss = 0.05047019\n",
            "Iteration 88, loss = 0.05127392\n",
            "Iteration 89, loss = 0.04992969\n",
            "Iteration 90, loss = 0.04865971\n",
            "Iteration 91, loss = 0.04780454\n",
            "Iteration 92, loss = 0.04699987\n",
            "Iteration 93, loss = 0.04646781\n",
            "Iteration 94, loss = 0.04468338\n",
            "Iteration 95, loss = 0.04357620\n",
            "Iteration 96, loss = 0.04310917\n",
            "Iteration 97, loss = 0.04229169\n",
            "Iteration 98, loss = 0.04116392\n",
            "Iteration 99, loss = 0.04045249\n",
            "Iteration 100, loss = 0.04007693\n",
            "Iteration 101, loss = 0.03916196\n",
            "Iteration 102, loss = 0.03850749\n",
            "Iteration 103, loss = 0.03797453\n",
            "Iteration 104, loss = 0.03791345\n",
            "Iteration 105, loss = 0.03911790\n",
            "Iteration 106, loss = 0.03715208\n",
            "Iteration 107, loss = 0.03525317\n",
            "Iteration 108, loss = 0.03594361\n",
            "Iteration 109, loss = 0.03445989\n",
            "Iteration 110, loss = 0.03451633\n",
            "Iteration 111, loss = 0.03429960\n",
            "Iteration 112, loss = 0.03282492\n",
            "Iteration 113, loss = 0.03323149\n",
            "Iteration 114, loss = 0.03286371\n",
            "Iteration 115, loss = 0.03247988\n",
            "Iteration 116, loss = 0.03071605\n",
            "Iteration 117, loss = 0.03133317\n",
            "Iteration 118, loss = 0.02937673\n",
            "Iteration 119, loss = 0.02922781\n",
            "Iteration 120, loss = 0.02855518\n",
            "Iteration 121, loss = 0.02807797\n",
            "Iteration 122, loss = 0.02775770\n",
            "Iteration 123, loss = 0.02725725\n",
            "Iteration 124, loss = 0.02687032\n",
            "Iteration 125, loss = 0.02682473\n",
            "Iteration 126, loss = 0.02663132\n",
            "Iteration 127, loss = 0.02589323\n",
            "Iteration 128, loss = 0.02528923\n",
            "Iteration 129, loss = 0.02514969\n",
            "Iteration 130, loss = 0.02447083\n",
            "Iteration 131, loss = 0.02424944\n",
            "Iteration 132, loss = 0.02385931\n",
            "Iteration 133, loss = 0.02393839\n",
            "Iteration 134, loss = 0.02318645\n",
            "Iteration 135, loss = 0.02317950\n",
            "Iteration 136, loss = 0.02272637\n",
            "Iteration 137, loss = 0.02282068\n",
            "Iteration 138, loss = 0.02225315\n",
            "Iteration 139, loss = 0.02226165\n",
            "Iteration 140, loss = 0.02232754\n",
            "Iteration 141, loss = 0.02219288\n",
            "Iteration 142, loss = 0.02252696\n",
            "Iteration 143, loss = 0.02083189\n",
            "Iteration 144, loss = 0.02109000\n",
            "Iteration 145, loss = 0.02013781\n",
            "Iteration 146, loss = 0.01998209\n",
            "Iteration 147, loss = 0.02062220\n",
            "Iteration 148, loss = 0.02016389\n",
            "Iteration 149, loss = 0.01913433\n",
            "Iteration 150, loss = 0.01872768\n",
            "Iteration 151, loss = 0.01805095\n",
            "Iteration 152, loss = 0.01776325\n",
            "Iteration 153, loss = 0.01771399\n",
            "Iteration 154, loss = 0.01723878\n",
            "Iteration 155, loss = 0.01690966\n",
            "Iteration 156, loss = 0.01704992\n",
            "Iteration 157, loss = 0.01691229\n",
            "Iteration 158, loss = 0.01628559\n",
            "Iteration 159, loss = 0.01617678\n",
            "Iteration 160, loss = 0.01580291\n",
            "Iteration 161, loss = 0.01578200\n",
            "Iteration 162, loss = 0.01534433\n",
            "Iteration 163, loss = 0.01531618\n",
            "Iteration 164, loss = 0.01506323\n",
            "Iteration 165, loss = 0.01497482\n",
            "Iteration 166, loss = 0.01468381\n",
            "Iteration 167, loss = 0.01463523\n",
            "Iteration 168, loss = 0.01469758\n",
            "Iteration 169, loss = 0.01408633\n",
            "Iteration 170, loss = 0.01407969\n",
            "Iteration 171, loss = 0.01390990\n",
            "Iteration 172, loss = 0.01359098\n",
            "Iteration 173, loss = 0.01360095\n",
            "Iteration 174, loss = 0.01330193\n",
            "Iteration 175, loss = 0.01312868\n",
            "Iteration 176, loss = 0.01285666\n",
            "Iteration 177, loss = 0.01291458\n",
            "Iteration 178, loss = 0.01255454\n",
            "Iteration 179, loss = 0.01246266\n",
            "Iteration 180, loss = 0.01255273\n",
            "Iteration 181, loss = 0.01207332\n",
            "Iteration 182, loss = 0.01205596\n",
            "Iteration 183, loss = 0.01213545\n",
            "Iteration 184, loss = 0.01194774\n",
            "Iteration 185, loss = 0.01171722\n",
            "Iteration 186, loss = 0.01160821\n",
            "Iteration 187, loss = 0.01127773\n",
            "Iteration 188, loss = 0.01129001\n",
            "Iteration 189, loss = 0.01120067\n",
            "Iteration 190, loss = 0.01099950\n",
            "Iteration 191, loss = 0.01082781\n",
            "Iteration 192, loss = 0.01059016\n",
            "Iteration 193, loss = 0.01089728\n",
            "Iteration 194, loss = 0.01083692\n",
            "Iteration 195, loss = 0.01025764\n",
            "Iteration 196, loss = 0.01035255\n",
            "Iteration 197, loss = 0.01004220\n",
            "Iteration 198, loss = 0.00994656\n",
            "Iteration 199, loss = 0.00972269\n",
            "Iteration 200, loss = 0.00975578\n",
            "Iteration 201, loss = 0.00967217\n",
            "Iteration 202, loss = 0.00953772\n",
            "Iteration 203, loss = 0.00960126\n",
            "Iteration 204, loss = 0.00934572\n",
            "Iteration 205, loss = 0.00907014\n",
            "Iteration 206, loss = 0.00908284\n",
            "Iteration 207, loss = 0.00891768\n",
            "Iteration 208, loss = 0.00894618\n",
            "Iteration 209, loss = 0.00873252\n",
            "Iteration 210, loss = 0.00853926\n",
            "Iteration 211, loss = 0.00852899\n",
            "Iteration 212, loss = 0.00852501\n",
            "Iteration 213, loss = 0.00833045\n",
            "Iteration 214, loss = 0.00842553\n",
            "Iteration 215, loss = 0.00820659\n",
            "Iteration 216, loss = 0.00801379\n",
            "Iteration 217, loss = 0.00803066\n",
            "Iteration 218, loss = 0.00788003\n",
            "Iteration 219, loss = 0.00783114\n",
            "Iteration 220, loss = 0.00774154\n",
            "Iteration 221, loss = 0.00759857\n",
            "Iteration 222, loss = 0.00751306\n",
            "Iteration 223, loss = 0.00753107\n",
            "Iteration 224, loss = 0.00757821\n",
            "Iteration 225, loss = 0.00728351\n",
            "Iteration 226, loss = 0.00722269\n",
            "Iteration 227, loss = 0.00710838\n",
            "Iteration 228, loss = 0.00716901\n",
            "Iteration 229, loss = 0.00694492\n",
            "Iteration 230, loss = 0.00691723\n",
            "Iteration 231, loss = 0.00679187\n",
            "Iteration 232, loss = 0.00675626\n",
            "Iteration 233, loss = 0.00677943\n",
            "Iteration 234, loss = 0.00661466\n",
            "Iteration 235, loss = 0.00655756\n",
            "Iteration 236, loss = 0.00655353\n",
            "Iteration 237, loss = 0.00640401\n",
            "Iteration 238, loss = 0.00656859\n",
            "Iteration 239, loss = 0.00629612\n",
            "Iteration 240, loss = 0.00631553\n",
            "Iteration 241, loss = 0.00624137\n",
            "Iteration 242, loss = 0.00614392\n",
            "Iteration 243, loss = 0.00600852\n",
            "Iteration 244, loss = 0.00597914\n",
            "Iteration 245, loss = 0.00588754\n",
            "Iteration 246, loss = 0.00575035\n",
            "Iteration 247, loss = 0.00573369\n",
            "Iteration 248, loss = 0.00565660\n",
            "Iteration 249, loss = 0.00558690\n",
            "Iteration 250, loss = 0.00555388\n",
            "Iteration 251, loss = 0.00553860\n",
            "Iteration 252, loss = 0.00544629\n",
            "Iteration 253, loss = 0.00535369\n",
            "Iteration 254, loss = 0.00531546\n",
            "Iteration 255, loss = 0.00527121\n",
            "Iteration 256, loss = 0.00525196\n",
            "Iteration 257, loss = 0.00528970\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "0.9722222222222222\n"
          ]
        }
      ]
    }
  ]
}